{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from analytics_utils.util.qubole import run_hive_as_spark, run_spark\n",
    "from analytics_utils.ml.preprocessing import StringToSparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import show\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import re\n",
    "import time\n",
    "#import xgboost as xgb\n",
    "from __builtin__ import round\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "core_consumer_token = 'SCj4GyFXjxSvrPass6rhXx96ZNHHa7mtqh6nhj3qvKa3xzBrzKpe4SyEPVEPxfkF'\n",
    "ids_cluster = 'spark_default'\n",
    "spark_cli = '--num-executors 80 --executor-cores 5  --executor-memory 18G --conf spark.rdd.compress=true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics_utils.util.qubole import run_hive\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM features.fancyfeast_julydata\n",
    "WHERE rand() < 0.5;\n",
    "\"\"\"\n",
    "\n",
    "df = run_hive(query, job_name = \"fancyfest_sample\", return_data=True, token = core_consumer_token, cluster = ids_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##standardize column names\n",
    "df.columns = ['hhid', 'flg', 'app_day_count', 'app_event_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"The total number of records is 2200216\"\n",
    "print \"The positive flg ratio in this subsample is \" + str(100.0*sum(df.flg)/df.shape[0]) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"The positive flg ratio in the test set is \" + str(100.0*sum(df.flg)/df.shape[0]) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Semi-Colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df.columns if col not in [\"hhid\", \"flg\"]]\n",
    "'''dataframe with columns 'app_day_count' and 'app_event_count'''\n",
    "X = df.reset_index(drop=True).ix[:, cols]\n",
    "'''label'''\n",
    "Y = df.reset_index(drop=True).flg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_colon_remove(X):\n",
    "    for i in range(0, 2):\n",
    "        a = X.iloc[:,i]\n",
    "        pointer = -1\n",
    "        for row in a:\n",
    "            pointer += 1\n",
    "            for m in re.finditer(':', row):\n",
    "                if row[m.start()+1] not in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "                    X.iloc[:, i][pointer] = a[pointer][:m.start()] + \"*\" + a[pointer][m.start()+1:]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = semi_colon_remove(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split And Transform Data (StringToSparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transform(X, Y, testset_ratio, seed):\n",
    "    '''data split'''\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=testset_ratio, random_state=seed)\n",
    "    '''data transform(string to sparse)'''\n",
    "    cols = list(X)\n",
    "    X_train_sparse = []\n",
    "    X_test_sparse = []\n",
    "    colnames = []\n",
    "    for col in cols:\n",
    "        delim_vec = StringToSparse(delim_1 = \",\", delim_2 = \":\")\n",
    "        X_train_sparse.append(delim_vec.fit_transform(X_train[col].as_matrix()))\n",
    "        X_test_sparse.append(delim_vec.transform(X_test[col].as_matrix()))\n",
    "        colnames_temp = [i + \"_\" + col for i in delim_vec.transformer.get_feature_names()]\n",
    "        colnames.append(colnames_temp)\n",
    "    X_train_sparse = sp.csr_matrix(sp.hstack(X_train_sparse))\n",
    "    X_test_sparse = sp.csr_matrix(sp.hstack(X_test_sparse))\n",
    "    Y_train = np.array(Y_train)\n",
    "    Y_test = np.array(Y_test)\n",
    "    colnames = np.array(colnames).flatten()\n",
    "    return X_train_sparse, X_test_sparse, Y_train, Y_test, colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset_ratio=0.3\n",
    "seed=820\n",
    "X_train_sparse, X_test_sparse, Y_train, Y_test, colnames = split_transform(X, Y, testset_ratio=testset_ratio, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_selection(X_train, X_test, Y_train, colnames, stability_selection = False):\n",
    "    %matplotlib inline\n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "    from sklearn.linear_model import RandomizedLogisticRegression\n",
    "    from IPython.display import display\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    print \"Total # of parameters: \" + str(len(colnames))\n",
    "    \n",
    "    '''STEP1: Remove Features with 0 Variance'''\n",
    "    print \"STEP1: Remove Features with 0 Variance\"\n",
    "    selector1 = VarianceThreshold(threshold=0)\n",
    "    X1 = selector1.fit_transform(X_train)\n",
    "    colnames1 = np.array(colnames)[selector1.get_support()]\n",
    "    print \"Total # of parameters: \" + str(len(colnames1))\n",
    "\n",
    "    '''STEP2: Remove Sparse Features'''\n",
    "    print \"STEP2: Remove Sparse Features\"\n",
    "    occurence_list = sp.csr_matrix.getnnz(X1, axis=0)\n",
    "    threshold = sum(occurence_list)/float(len(occurence_list))/10\n",
    "    keep_index = np.where(occurence_list >= threshold)[0].tolist()\n",
    "    X2 = X1[:, keep_index]\n",
    "    colnames2 = colnames1[keep_index]\n",
    "    print \"Total #of parameters: \"+ str(len(colnames2))\n",
    "    \n",
    "    '''STEP3: Filter Method: Univariate Selection using Chi2'''\n",
    "    print \"STEP3: Filter Method: Univariate Selection using Chi2\"\n",
    "    cutoff = (chi2(X1, Y_train)[1] < 0.05).sum()\n",
    "    \n",
    "    plt.plot(sorted(chi2(X1, Y_train)[1]))\n",
    "    plt.axvline(x=cutoff)\n",
    "    plt.show()\n",
    "    \n",
    "    selector3 = SelectKBest(score_func=chi2, k=cutoff)\n",
    "    X3 = selector3.fit_transform(X2, Y_train)\n",
    "    colnames3 = np.array(colnames2)[selector3.get_support()]    \n",
    "    print \"Total # of parameters: \" + str(len(colnames3))    \n",
    "    \n",
    "    if(stability_selection == True):\n",
    "        '''STEP4: Stability Selection using Randomized Logistic Regression'''\n",
    "        print \"STEP4: Stability Selection using Randomized Logistic Regression\"\n",
    "        selector4 = RandomizedLogisticRegression(selection_threshold=0.0, random_state=seed)\n",
    "        selector4.fit(X3, Y_train)\n",
    "        X4 = selector4.transform(X3.toarray())\n",
    "        colnames4 = np.array(colnames3)[selector4.get_support()]    \n",
    "        print \"Features sorted by their score:\"\n",
    "        non_zero_features = selector4.scores_[selector4.scores_ != 0]\n",
    "        print sorted(zip(map(lambda x: round(x, 4), non_zero_features), colnames4), reverse=True)\n",
    "        print \"Total # of parameters: \" + str(len(colnames4)) \n",
    "    \n",
    "    '''FINAL STEP: Transform X_test'''\n",
    "    print \"start test transformation\"\n",
    "    X_test1 = selector1.transform(X_test)\n",
    "    print \"finish 1st transformation\"\n",
    "    X_test2 = X_test1[:, keep_index]\n",
    "    print \"finish 2nd transformation\"\n",
    "    X_test3 = selector3.transform(X_test2)\n",
    "    print \"finish 3rd transformation\"\n",
    "    #X_test4 = selector4.transform(X_test3)\n",
    "    #print \"finish 4th transformation\"\n",
    "    #return X4, X_test4, colnames4\n",
    "    return X3, X_test3, colnames3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "pcorr = chi2(X_train_sparse, Y_train)[1]\n",
    "sparsity = sp.csr_matrix.getnnz(X_train_sparse, axis=0)[::-1]/float(X_train_sparse.shape[0])*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoway_df = pd.DataFrame({'pcorr': pcorr, 'sparsity': sparsity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_selection(X_train, X_test, Y_train, colnames, stability_selection = False):\n",
    "    %matplotlib inline\n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "    from sklearn.linear_model import RandomizedLogisticRegression\n",
    "    from IPython.display import display\n",
    "    \n",
    "    print \"Total # of parameters: \" + str(len(colnames))\n",
    "    \n",
    "    '''STEP1: Remove Features with 0 Variance'''\n",
    "    print \"STEP1: Remove Features with 0 Variance\"\n",
    "    selector1 = VarianceThreshold(threshold=0)\n",
    "    X1 = selector1.fit_transform(X_train)\n",
    "    colnames1 = np.array(colnames)[selector1.get_support()]\n",
    "    print \"Total # of parameters: \" + str(len(colnames1))\n",
    "\n",
    "    '''STEP2: Filter Method: Univariate Selection using Chi2'''\n",
    "    print \"STEP2: Filter Method: Univariate Selection using Chi2\"\n",
    "    #cutoff = (chi2(X1, Y_train)[1] < 0.05).sum()\n",
    "    \n",
    "    #plt.plot(sorted(chi2(X1, Y_train)[1]))\n",
    "    #plt.axvline(x=cutoff)\n",
    "    #plt.show()\n",
    "    \n",
    "    chi2(X1, Y_train)[1]\n",
    "    selector2 = SelectKBest(score_func=chi2, k=5)\n",
    "    X2 = selector2.fit_transform(X1, Y_train)\n",
    "    colnames2 = np.array(colnames1)[selector2.get_support()]    \n",
    "    print \"Total # of parameters: \" + str(len(colnames2))    \n",
    "    \n",
    "    if(stability_selection == True):\n",
    "        '''STEP3: Stability Selection using Randomized Logistic Regression'''\n",
    "        print \"STEP3: Stability Selection using Randomized Logistic Regression\"\n",
    "        selector3 = RandomizedLogisticRegression(selection_threshold=0.0, random_state=seed)\n",
    "        selector3.fit(X2, Y_train)\n",
    "        X3 = selector3.transform(X2.toarray())\n",
    "        colnames3 = np.array(colnames2)[selector3.get_support()]    \n",
    "        print \"Features sorted by their score:\"\n",
    "        non_zero_features = selector3.scores_[selector3.scores_ != 0]\n",
    "        print sorted(zip(map(lambda x: round(x, 4), non_zero_features), colnames3), reverse=True)\n",
    "        print \"Total # of parameters: \" + str(len(colnames3)) \n",
    "    \n",
    "    '''FINAL STEP: Transform X_test'''\n",
    "    print \"start test transformation\"\n",
    "    X_test1 = selector1.transform(X_test)\n",
    "    print \"finish 1st transformation\"\n",
    "    X_test2 = selector2.transform(X_test1)\n",
    "    print \"finish 2nd transformation\"\n",
    "    #X_test3 = selector3.transform(X_test2)\n",
    "    #return X3, X_test3, colnames3\n",
    "    return X2, X_test2, colnames2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train models ##\n",
    "import copy\n",
    "from analytics_utils.ml.modeling import train_class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "## sparse models ##\n",
    "grid = {\n",
    "    \"model__penalty\":['l1','l2']\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('model',LogisticRegression(n_jobs=8))\n",
    "])\n",
    "\n",
    "predictions, model_logistic = train_class(model=copy.deepcopy(pipe), grid=grid, train=X_train_sparse, target=Y_train, \\\n",
    "                                          cv=cv)\n",
    "preds_logistic = model_logistic.predict_prob(X_test_sparse)\n",
    "print roc_auc_score(Y_test, preds_logistic[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(Y_test, preds_logistic[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_logistic = model_logistic.predict_proba(X_test_sparse)\n",
    "#print roc_auc_score(Y_test, preds_logistic[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use All Features'''\n",
    "predictions, model_logistic = train_class(model=copy.deepcopy(pipe), grid=grid, train=X_train_sparse, target=Y_train, \\\n",
    "                                          cv=cv)\n",
    "preds_logistic = model_logistic.predict_proba(X_test_sparse)\n",
    "print roc_auc_score(Y_test, preds_logistic[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model_svm = svm.linearSVC()\n",
    "model_svm.fit(X_train_final, Y_train)\n",
    "preds_svm = model_svm.predict_prob(X_test_final)\n",
    "print roc_auc_score(Y_test, preds_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random_forest = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "model_random_forest.fit(X_train_final, Y_train)\n",
    "preds_random_forest = model_random_forest.predict_proba(X_test_final)\n",
    "roc_auc_score(Y_test, preds_random_forest[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_final, Y_train)\n",
    "dtest = xgb.DMatrix(X_test_final, Y_test)\n",
    "\n",
    "xgb_params = {\"n_jobs\":-1,\n",
    "              \"eval_metric\": \"auc\",\n",
    "              \"objective\": \"binary:logistic\",\n",
    "              \"eta\": 0.1,\n",
    "              \"max_depth\": 6,\n",
    "              \"min_child_weight\": 10,\n",
    "              \"gamma\": 0.7,\n",
    "              \"subsample\": 0.7,\n",
    "              \"colsample_bytree\" : 0.6,\n",
    "              \"alpha\": 0.1,\n",
    "              \"lambda\": 10,\n",
    "              \"max_delta_step\": 1,\n",
    "              \"seed\": 820}\n",
    "\n",
    "eval_set  = [(dtrain, \"train\"), (dtest, \"test\")]\n",
    "model_xgb = xgb.train(params=xgb_params, dtrain=dtrain, num_boost_round=100, evals=eval_set, verbose_eval=10, early_stopping_rounds = 1000)\n",
    "preds_xgb = model_xgb.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_plot(label=Y_test, pred=preds_random_forest[:, 1], top_segments=1, segments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_plot(label, pred, top_segments, segments):\n",
    "    '''Prediction Distribution'''\n",
    "    print \"Prediction Distribution\"\n",
    "    plt.hist(pred)\n",
    "    \n",
    "    '''AUC & Precision Recall'''\n",
    "    print \"AUC & Precision Recall\"\n",
    "    auc_precision_recall(label, pred)\n",
    "    \n",
    "    '''Lift Table'''\n",
    "    print \"Lift Table\"\n",
    "    lift_table(label, pred, top_segments, segments)\n",
    "    \n",
    "    '''Score Distribution'''\n",
    "    print \"Score Distribution\"\n",
    "    score_distribution(label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC & Precision Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_precision_recall(label, pred):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "    \n",
    "    '''ROC Plot'''\n",
    "    auc = roc_auc_score(label, pred)\n",
    "    fpr, tpr, thresh = roc_curve(label, pred)\n",
    "    ax1.plot(fpr, tpr, label=\"ROC, auc=\"+str(auc))\n",
    "    ax1.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "    ax1.axis(xmin = 0.0, xmax = 1.0, ymin = 0.0, ymax = 1.0)\n",
    "    ax1.set_xlabel('False Positive Rate or (1 - Specifity)')\n",
    "    ax1.set_ylabel('True Positive Rate or (Sensitivity)')\n",
    "    ax1.set_title('Receiver Operating Characteristic')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "    '''Precision Recall Plot'''\n",
    "    #compute average precision rescore\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    average_precision = average_precision_score(label, pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precision, recall, _ = precision_recall_curve(label, pred)\n",
    "    ax2.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    ax2.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.axis(xmin = 0.0, xmax = 1.0, ymin = 0.0, ymax = 1.05)\n",
    "    ax2.set_title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Precision & Lift Table & Cumulative Recall Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_table(label, pred, top_segments, segments):\n",
    "    df = pd.DataFrame({\"label\": label, \"pred\": pred})\n",
    "    df_sorted = df.sort_values('pred', ascending = False).reset_index().nlargest(int(round(0.2*len(df))),'pred')\n",
    "    segment_size = np.round(len(df_sorted)/segments)\n",
    "    pos_label = []\n",
    "    seg_churners_cum = []\n",
    "    for i in (np.array(range(segments)) + 1)*segment_size:\n",
    "        pos_label.append(df_sorted.loc[i - segment_size:i-1, 'label'].values.sum())\n",
    "    segment_percentile = np.linspace(0, top_segments, segments+1)*100\n",
    "    segment_percentile_1 = np.delete(segment_percentile, 0)\n",
    "    segment_percentile_2 = np.delete(segment_percentile, len(segment_percentile)-1)\n",
    "    \n",
    "    '''Top Precision'''\n",
    "    top_precision = df_sorted['label'].mean()\n",
    "    print \"The top\" + \"{:3.0f}%\".format(top_segments*100) + \" segments has\" + \"{:3.0f}%\".format(top_precision*100) + \" precision.\"\n",
    "    \n",
    "    '''Precision'''\n",
    "    segment_precision = [round(float(i)/segment_size, 3) for i in pos_label]\n",
    "    baseline = round(float(df_sorted['label'].sum())/len(df_sorted), 3)\n",
    "    \n",
    "    '''Cumulative Recall'''\n",
    "    cumulative_recall = [round(i, 3) for i in np.array(pos_label).cumsum()/float(df_sorted['label'].sum())]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "    ax1.plot(segment_percentile_1, segment_precision, 'b-', alpha = 0.5)\n",
    "    ax1.axhline(y=baseline, c=\"red\", linewidth=0.5, zorder=0, label = \"Baseline\")\n",
    "    vals = ax1.get_yticks()\n",
    "    ax1.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    ax1.set_xlabel('Segment Percentile')\n",
    "    ax1.set_ylabel('Segment Precision')\n",
    "    ax1.set_title('Proportion of Buyers by Segments')\n",
    "    ax1.legend()\n",
    "    \n",
    "    width = segment_percentile[1]-segment_percentile[0]\n",
    "    ax2.bar(segment_percentile_2, cumulative_recall, color = 'red', alpha = 0.5, width=width, align='edge')\n",
    "    vals2 = ax2.get_yticks()\n",
    "    ax2.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals2])\n",
    "    ax2.set_xlabel('Segment Percentile')\n",
    "    ax2.set_ylabel('Cumulative Recall')\n",
    "    ax2.set_title('Cumulative Proportion of Buyers')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_distribution(label, pred):\n",
    "    import bisect\n",
    "    import pylab as pl\n",
    "    \n",
    "    interval = pl.frange(0,1,0.01)\n",
    "    df = pd.DataFrame({'label':label, 'pred':pred})\n",
    "    df_0 = df[df['label'] == 0]\n",
    "    df_1 = df[df['label'] == 1]\n",
    "\n",
    "    '''Buyers'''\n",
    "    buckets_buyers = dict((i,0) for i in interval)\n",
    "    for i in df_1['pred']:\n",
    "        bucket = interval[bisect.bisect_left(interval, i)]\n",
    "        buckets_buyers[bucket] += 1\n",
    "    df_buyers = pd.DataFrame(buckets_buyers.items())\n",
    "    df_buyers.columns = ['Interval', 'Buyers']\n",
    "    df_buyers['Buyers'] = df_buyers['Buyers']/len(df_1)*100\n",
    "    df_buyers = df_buyers.sort_values('Interval')\n",
    "    \n",
    "    '''Non-Buyers'''\n",
    "    buckets_non_buyers = dict((i,0) for i in interval)\n",
    "    for i in df_0['pred']:\n",
    "        bucket = interval[bisect.bisect_left(interval, i)]\n",
    "        buckets_non_buyers[bucket] += 1\n",
    "    df_non_buyers = pd.DataFrame(buckets_non_buyers.items())\n",
    "    df_non_buyers.columns = ['Interval', 'Non-Buyers']\n",
    "    df_non_buyers['Non-Buyers'] = df_non_buyers['Non-Buyers']/len(df_0)*100\n",
    "    df_non_buyers = df_non_buyers.sort_values('Interval')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_buyers['Interval'], df_buyers['Buyers'], color = 'red', alpha = 0.5)\n",
    "    ax.plot(df_non_buyers['Interval'], df_non_buyers['Non-Buyers'], color = 'blue', alpha = 0.5)\n",
    "    vals = ax.get_yticks()\n",
    "    ax.set_yticklabels(['{:3.0f}%'.format(x) for x in vals])\n",
    "    ax.legend(['Buyers', 'Non-Buyers'])\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Score Distribution')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Evaluation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_plot(label, pred, top_segments, segments):\n",
    "    '''Prediction Distribution'''\n",
    "    print \"Metric 1 : Prediction Distribution\"\n",
    "    plt.hist(pred)\n",
    "    \n",
    "    '''AUC & Precision Recall'''\n",
    "    print \"Metric 2 : AUC & Precision Recall\"\n",
    "    auc_precision_recall(label, pred)\n",
    "    \n",
    "    '''Lift Table'''\n",
    "    print \"Metric 3 : Lift Table\"\n",
    "    lift_table(label, pred, top_segments, segments)\n",
    "    \n",
    "    '''Score Distribution'''\n",
    "    print \"Metric 4 : Score Distribution\"\n",
    "    score_distribution(label, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
